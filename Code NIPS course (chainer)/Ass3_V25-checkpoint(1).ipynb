{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "CGc6Y5_eKaPD"
   },
   "source": [
    "**SOW-MKI49: Neural Information Processing Systems**  \n",
    "*Weeks 4 and 5: Assignment (100 points + 20 bonus points)  \n",
    "Author: Luca and Umut"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "cBlzrHdoKzvY"
   },
   "outputs": [],
   "source": [
    "# Group number: 14\n",
    "# Veerle Schepers s1023102\n",
    "# Angeliki-Ilektra Karaiskou, s1029746\n",
    "# Lei Xiaoxuan, s1025681\n",
    "# Parsia Basimfar : s1022274"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Gz0FZAwiJ-al"
   },
   "outputs": [],
   "source": [
    "from chainer import cuda, datasets, serializers,optimizers\n",
    "from chainer.dataset import DatasetMixin, concat_examples\n",
    "from chainer.iterators import MultithreadIterator\n",
    "from chainer.functions.loss.vae import gaussian_kl_divergence\n",
    "import chainer\n",
    "import chainer.functions as F\n",
    "import chainer.links as L\n",
    "import cupy\n",
    "import numpy as np\n",
    "import tqdm\n",
    "import IPython\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Gz0FZAwiJ-al"
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import matplotlib\n",
    "import os\n",
    "import pickle\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ckO0T0SLAFf7"
   },
   "source": [
    "This is the decoder class. It transforms latents (features) to observables (images). It corresponds to p(x | z) in the context of variational inference (and the slides), where x is observables and y is latents.\n",
    "\n",
    "Task: (10 points)\n",
    "\n",
    "- Implement the decoder class for a variational autoencoder. Note that the decoder should output the Gaussian distribution parameters (mean and variance per pixel) of images rather than images themselves."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(chainer.ChainList):\n",
    "    def __init__(self): # <= you might want to pass some architecture parameters (e.g., #i/o units, etc.) here\n",
    "        # input  = z        = latents\n",
    "        \n",
    "        # output = mean_x   = observables mean\n",
    "        #        & ln_var_x = observables variance\n",
    "        \n",
    "        \n",
    "        super(Decoder, self).__init__(\n",
    "            # pass your decoder layers here\n",
    "            # p(z,x) = D_p(x | g_p(z))p(z)\n",
    "            \n",
    "            #lin_dec_1 = \n",
    "            L.Linear(100,300),\n",
    "            #lin_dec_2 =\n",
    "            L.Linear(300,500),\n",
    "            #lin_dec_mu     =\n",
    "            L.Linear(500, 784),\n",
    "            #lin_dec_ln_var = \n",
    "            L.Linear(500, 784)\n",
    "\n",
    "        )\n",
    "\n",
    "    def __call__(self, x):\n",
    "        h1_dec = self[0](x)\n",
    "        h1_dec = F.relu(h1_dec)\n",
    "        h2_dec = self[1](h1_dec)\n",
    "        h2_dec = F.relu(h2_dec)\n",
    "        \n",
    "        h_dec_mu = self[2](h2_dec)\n",
    "        h_dec_mu = F.sigmoid(h_dec_mu)\n",
    "        h_dec_ln_var = self[3](h2_dec)\n",
    "        h_dec_ln_var = F.sigmoid(h_dec_ln_var)\n",
    "        \n",
    "        \n",
    "        return h_dec_mu, h_dec_ln_var"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "d36RjWO6AKqO"
   },
   "source": [
    "This is the encoder class. It transforms observables (images) to latents (features). It corresponds to q(z | x) in the context of variational inference (and the slides), where z is latents and x is observables.\n",
    "\n",
    "Task: (10 points)\n",
    "\n",
    "- Implement the encoder class for a variational autoencoder. Note that the encoder should output the Gaussian distribution parameters (mean and variance per feature) of features rather than features themselves."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(chainer.ChainList): #Q(z|X)\n",
    "    def __init__(self): \n",
    "        # <= you might want to pass some architecture parameters (e.g., #i/o units, etc.) here\n",
    "        # input = image = x\n",
    "        # output = mean_z and variance_z\n",
    "        \n",
    "        # MNIST images have a dimension of 28 * 28 pixels with one color channel\n",
    "        # 28*28*1 = 784\n",
    "        \n",
    "        \n",
    "        super(Encoder, self).__init__(\n",
    "            # pass your encoder layers here\n",
    "            # q(z,x) = D_q((z | h_q(x))k(x))\n",
    "            \n",
    "            # hidden layers\n",
    "            # 3 Layered ReLu 784 - 500 - 300 - 100\n",
    "            \n",
    "            #l0in_enc_1 = \n",
    "            L.Linear(784,500),\n",
    "            #lin_enc_2 = \n",
    "            L.Linear(500,300),\n",
    "            #lin_enc_mu     = \n",
    "            L.Linear(300,100),\n",
    "            #lin_enc_ln_var =\n",
    "            L.Linear(300,100)        \n",
    "\n",
    "        )\n",
    "\n",
    "    def __call__(self, x):    \n",
    "        h1_enc = self[0](x)\n",
    "        h1_enc = F.relu(h1_enc)\n",
    "        h2_enc = self[1](h1_enc)\n",
    "        h2_enc = F.relu(h2_enc)        \n",
    "        h_enc_mu = self[2](h2_enc)\n",
    "        h_enc_ln_var = self[3](h2_enc)\n",
    "\n",
    "        return h_enc_mu, h_enc_ln_var"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "8ch2iPFbBWVu"
   },
   "source": [
    "This is the loss class. The loss of encoder and decoder of a variational autoencoder is the evidence lower bound as follows:\n",
    "\n",
    "$L = D_{KL}(q(z | x), p(z)) -  E_{z\\sim q}[log p(x | z)]$\n",
    "\n",
    "The first term above is the KL divergence between the approximate posterior (q) and the prior (p), which can be interpreted as a form of regularization. You can assume that the prior is unit Gaussian. It can be implemented with the F.gaussian_kl_divergence function in Chainer.\n",
    "\n",
    "The second term above is the Gaussian negative log likelihood. This is the term that fits the data, which is very similar to the usual loss functions that you use in deep learning. It can be implemented with the F.gaussian_nll function in Chainer.\n",
    "\n",
    "Task: \n",
    "\n",
    "- Implement the loss class. (10 points)\n",
    "\n",
    "As input, it gets the following arguments:\n",
    "\n",
    "mean_y => mean of the encoded features (output of the encoder)  \n",
    "ln_var_y => log variance of the encoded features (output of the encoder)  \n",
    "x => input images (mini batch)  \n",
    "mean_x => mean of the decoded images (output of the decoder)  \n",
    "ln_var_x => mean of the decoded images (output of the decoder)  \n",
    "\n",
    "As output, it gives the loss.\n",
    "\n",
    "- Explain why we use log variance instead of variance. (5 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "iN6gKfG9BWfC"
   },
   "outputs": [],
   "source": [
    "class Loss(object):\n",
    "    def __call__(self, mean_z, ln_var_z, x, mean_x, ln_var_x):\n",
    "        generation_loss = F.gaussian_nll(x, mean_x, ln_var_x)\n",
    "        latent_loss = F.gaussian_kl_divergence(mean_z, ln_var_z)\n",
    "        loss = generation_loss + latent_loss\n",
    "        return loss "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Explain why we use log variance instead of variance. (5 points)\n",
    "\n",
    "# Since the log variance is a function that monotonically increases by its argument,\n",
    "# maximazing of the log variance is equal to maximizing the function itself.\n",
    "# When using the log variance we can use a sum instead of a product which makes it easier to use it in computations.\n",
    "# thus, it simplifies the mathimatical analysis.\n",
    "\n",
    "# Also, taking the product of a great amount of small probabilities can casue underflow in the numerical precision.\n",
    "# This is not the case when the sum of the logarithmic probabilities is used.\n",
    "# Thus, it leads to a more precise answer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "wuS4ynBqBYt9"
   },
   "source": [
    "This is the model class. It combines the encoder and the decoder.\n",
    "\n",
    "Task: (20 points)\n",
    "\n",
    "- Implement the reparameterziation trick for sampling latents. (10 points)\n",
    "- Explain why we need to use this trick. (10 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "kTLyWlw2BY3H"
   },
   "outputs": [],
   "source": [
    "class Model(chainer.Chain):\n",
    "    def __init__(self, decoder, encoder):\n",
    "        super(Model, self).__init__()\n",
    "\n",
    "        with self.init_scope():\n",
    "            self.decoder = decoder\n",
    "            self.encoder = encoder\n",
    "\n",
    "    def __call__(self, x):\n",
    "        ln_var_z, mean_z = self.encoder(x)\n",
    "        \n",
    "        # Sample latents (z) from the Gaussian with parameters ln_var_z, mean_z by using the reparameterization trick\n",
    "        # z = mean_z + ln_var_z * epsilon\n",
    "        # epsilon is a fixed stocastic node, standard gaussian (mean = 0, sd = 1)\n",
    "        \n",
    "        # Define eps\n",
    "        eps = cupy.random.normal(loc=0.0, scale=1.0) # or \n",
    "        #eps = np.random.normal(0,1) \n",
    "        # calculate sigma_z from ln_var_z\n",
    "        sigma_z = F.exp(ln_var_z / 2)\n",
    "        # calculate sampled latent vector z\n",
    "        z = mean_z + sigma_z * eps\n",
    "        #print(\"z = \")\n",
    "        #print(z)\n",
    "        ln_var_x, mean_x = self.decoder(z)\n",
    "        #print(\"mean_x\")\n",
    "        #print(mean_x)\n",
    "\n",
    "        return mean_z, ln_var_z, mean_x, ln_var_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Explain why we need to use this trick. (10 points)\n",
    "\n",
    "# We need to use the reparameterization trick because we cannot run backpropagation without it due to the sampled latent vector.\n",
    "# This is impossible since the sampled latent vector is a stocastic node which blocks the back propagation.\n",
    "# The reparameterization trick adds another node next to the mean and the vaiance, namely epsilon.\n",
    "# Epsilon is gonna be standard gaussian, so it has mean = 0 and sd = 1.\n",
    "# We now can sammple from epsilon, multiply this with sigma, and add mu.\n",
    "# So instead of having a full stocactis node z that blocks backpropagation\n",
    "# We split it up into a part where we can do backpropagation\n",
    "# and another part that is still stocastic (epsilon)\n",
    "# but we don't want to train epsilon since it is fixed, this is not a problem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "6UYXDr2-HrrA"
   },
   "source": [
    "This is a helper class to use the Mnist dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "12AV4tiBHo0W"
   },
   "outputs": [],
   "source": [
    "class Mnist(DatasetMixin):\n",
    "    def __init__(self):\n",
    "        self.dataset = datasets.get_mnist(False)[0 if chainer.config.train else 1]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "\n",
    "    def get_example(self, i):\n",
    "        return self.dataset[i]\n",
    "    \n",
    "    # The MNIST database is a dataset of handwritten digits. \n",
    "    # It has 60,000 training samples, and 10,000 test samples. \n",
    "    # Each image is represented by 28x28 pixels, \n",
    "    # each containing a value 0 - 255 with its grayscale value."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "tC5HdWquIw7B"
   },
   "source": [
    "Task: (50 points)\n",
    "\n",
    "- Train the above defined variational autoencoder on the Mnist dataset. You can refer to the earlier assignments to implement your training loop. (25 points)\n",
    "\n",
    "- How good are the samples? Randomy sample some digits and visualize them. (10 points)\n",
    "\n",
    "- How good are the reconstructions? Draw an Mnist like digit, encode it, decode it and visualize the digits. How different is the reconstruction from the original. (10 points)\n",
    "\n",
    "- Repeat the last task but by drawing something other than a digit (e.g., a face). How accuracte is the reconstructions? Explain the results. (5 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VAE(object):\n",
    "    def __init__(self): # Here I will add more things\n",
    "        self.log = {('test', 'accuracy'): (), ('test', 'loss'): (), ('training', 'accuracy'): (),\n",
    "                    ('training', 'loss'): ()}\n",
    "        self.encoder=Encoder()\n",
    "        self.decoder=Decoder()\n",
    "        self.model = Model(self.decoder,self.encoder)\n",
    "        self.optimizer = optimizers.Adam(0.0002, 0.5)\n",
    "        self.optimizer.setup(self.model)\n",
    "        self.loss =Loss()\n",
    "\n",
    "    def __call__(self, x):\n",
    "        mean_z, ln_var_z, mean_x, ln_var_x = self.model(x)\n",
    "        \n",
    "        return mean_z, ln_var_z, mean_x, ln_var_x\n",
    "\n",
    "    @classmethod\n",
    "    def load(cls, directory):\n",
    "        self = cls()\n",
    "        self.log = np.load('{}/log.npy'.format(directory))\n",
    "        # Load model\n",
    "        serializers.load_npz('{}/model.npz'.format(directory), self.model)\n",
    "        serializers.load_npz('{}/optimizer.npz'.format(directory), self.optimizer)\n",
    "        \n",
    "        return self\n",
    "\n",
    "    def save(self, directory):\n",
    "        np.save('{}/log.npy'.format(directory), self.log)\n",
    "        # Save model\n",
    "        serializers.save_npz('{}/model.npz'.format(directory), self.model)\n",
    "        serializers.save_npz('{}/optimizer.npz'.format(directory), self.optimizer)\n",
    "\n",
    "    def test(self, x):\n",
    "        with chainer.using_config('train', False):\n",
    "            # Forward prop\n",
    "            # Forward prop     \n",
    "            mean_z, ln_var_z, mean_x, ln_var_x = self.model(x)\n",
    "            loss = self.loss(mean_z, ln_var_z, x, mean_x, ln_var_x)\n",
    "            #self.log['test', 'accuracy'] += (float(F.accuracy(Q_hat, Q).data),)\n",
    "            self.log['test', 'loss'] += (float(loss.data),)\n",
    "            \n",
    "            return mean_z, ln_var_z, mean_x, ln_var_x \n",
    "\n",
    "    def train(self,x):\n",
    "        # same as in test\n",
    "        mean_z, ln_var_z, mean_x, ln_var_x = self.model(x)\n",
    "        loss = self.loss(mean_z, ln_var_z, x, mean_x, ln_var_x)   \n",
    "        # cleargrads() -- Clears all gradient arrays.\n",
    "            # This method should be called before the backward computation at every iteration of the optimization.\n",
    "        # backprop() -- Backpropagation works by using a loss function to calculate how far the network was from the target output.\n",
    "        # optimizer.update() -- Updates the parameters.\n",
    "        \n",
    "        # use self to get the model and optimizer from the init\n",
    "        \n",
    "        self.model.cleargrads()\n",
    "        loss.backward()\n",
    "        self.optimizer.update()\n",
    "\n",
    "        self.log['training', 'loss'] += (float(loss.data),)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "INITIALIZATION\n",
    "-------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "I34eP98AIV-j"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<chainer.optimizers.adam.Adam at 0x23953d8f5c0>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from chainer import iterators, dataset\n",
    "\n",
    "Dataset = Mnist()\n",
    "device = 0\n",
    "loss_fn = Loss()\n",
    "batch_size = 64\n",
    "model_directory = 'Model'\n",
    "#model_directory = 'C:\\\\Users\\\\Αngeliki-Ilektra\\\\NeuralInformationProcessingSystems\\\\3rdAs\\\\modelDirectory'\n",
    "model = Model(Decoder(), Encoder()).to_gpu(device)\n",
    "#vae = VAE()\n",
    "#root=\n",
    "training_set = Dataset[:int(0.8*55000)]\n",
    "validation_set = Dataset[int(0.8*55000):55000]\n",
    "#validation_set = Dataset[int(0.01*55000):int(0.011*55000)]\n",
    "test_set = Dataset[55000:len(Dataset)]\n",
    "\n",
    "loss_history = {'training': [], 'validation': []}\n",
    "\n",
    "\n",
    "training_iterator = iterators.SerialIterator(training_set, batch_size, False, True)\n",
    "#print(enumerate(training_iterator))\n",
    "validation_iterator = iterators.SerialIterator(validation_set, batch_size, False, False)\n",
    "test_iterator = iterators.SerialIterator(test_set, batch_size, False, False)\n",
    "\n",
    "#vae.model.to_cpu()\n",
    "#epochs=2\n",
    "optimizer = chainer.optimizers.Adam(0.0004)\n",
    "optimizer.setup(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TRAINING\n",
    "-------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "I34eP98AIV-j"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration i = \n",
      "0\n",
      "is ended and we are going to the next one\n",
      "-------------------------------------------\n",
      "epoch:   1 / 050, training loss: 48083.0586 , validation loss: 47178.4802.\n",
      "iteration i = \n",
      "1\n",
      "is ended and we are going to the next one\n",
      "-------------------------------------------\n",
      "epoch:   2 / 050, training loss: 47006.5895 , validation loss: 46906.6103.\n",
      "iteration i = \n",
      "2\n",
      "is ended and we are going to the next one\n",
      "-------------------------------------------\n",
      "epoch:   3 / 050, training loss: 46767.9022 , validation loss: 46764.9023.\n",
      "iteration i = \n",
      "3\n",
      "is ended and we are going to the next one\n",
      "-------------------------------------------\n",
      "epoch:   4 / 050, training loss: 46682.2168 , validation loss: 46657.6344.\n",
      "iteration i = \n",
      "4\n",
      "is ended and we are going to the next one\n",
      "-------------------------------------------\n",
      "epoch:   5 / 050, training loss: 46594.6632 , validation loss: 46590.3595.\n",
      "iteration i = \n",
      "5\n",
      "is ended and we are going to the next one\n",
      "-------------------------------------------\n",
      "epoch:   6 / 050, training loss: 46542.3801 , validation loss: 46553.6847.\n",
      "iteration i = \n",
      "6\n",
      "is ended and we are going to the next one\n",
      "-------------------------------------------\n",
      "epoch:   7 / 050, training loss: 46507.7320 , validation loss: 46529.7597.\n",
      "iteration i = \n",
      "7\n",
      "is ended and we are going to the next one\n",
      "-------------------------------------------\n",
      "epoch:   8 / 050, training loss: 46471.4644 , validation loss: 46476.2262.\n",
      "iteration i = \n",
      "8\n",
      "is ended and we are going to the next one\n",
      "-------------------------------------------\n",
      "epoch:   9 / 050, training loss: 46466.4147 , validation loss: 46468.2974.\n",
      "iteration i = \n",
      "9\n",
      "is ended and we are going to the next one\n",
      "-------------------------------------------\n",
      "epoch:  10 / 050, training loss: 46465.8030 , validation loss: 46478.3378.\n",
      "iteration i = \n",
      "10\n",
      "is ended and we are going to the next one\n",
      "-------------------------------------------\n",
      "epoch:  11 / 050, training loss: 46421.6100 , validation loss: 46490.6531.\n",
      "iteration i = \n",
      "11\n",
      "is ended and we are going to the next one\n",
      "-------------------------------------------\n",
      "epoch:  12 / 050, training loss: 46396.3708 , validation loss: 46445.5842.\n",
      "iteration i = \n",
      "12\n",
      "is ended and we are going to the next one\n",
      "-------------------------------------------\n",
      "epoch:  13 / 050, training loss: 46384.5001 , validation loss: 46434.7321.\n",
      "iteration i = \n",
      "13\n",
      "is ended and we are going to the next one\n",
      "-------------------------------------------\n",
      "epoch:  14 / 050, training loss: 46416.2827 , validation loss: 46368.4734.\n",
      "iteration i = \n",
      "14\n",
      "is ended and we are going to the next one\n",
      "-------------------------------------------\n",
      "epoch:  15 / 050, training loss: 46362.4235 , validation loss: 46349.4653.\n",
      "iteration i = \n",
      "15\n",
      "is ended and we are going to the next one\n",
      "-------------------------------------------\n",
      "epoch:  16 / 050, training loss: 46373.8455 , validation loss: 46362.4592.\n",
      "iteration i = \n",
      "16\n",
      "is ended and we are going to the next one\n",
      "-------------------------------------------\n",
      "epoch:  17 / 050, training loss: 46380.3305 , validation loss: 46342.3549.\n",
      "iteration i = \n",
      "17\n",
      "is ended and we are going to the next one\n",
      "-------------------------------------------\n",
      "epoch:  18 / 050, training loss: 46352.8998 , validation loss: 46347.4920.\n",
      "iteration i = \n",
      "18\n",
      "is ended and we are going to the next one\n",
      "-------------------------------------------\n",
      "epoch:  19 / 050, training loss: 46355.2563 , validation loss: 46341.2742.\n",
      "iteration i = \n",
      "19\n",
      "is ended and we are going to the next one\n",
      "-------------------------------------------\n",
      "epoch:  20 / 050, training loss: 46355.5783 , validation loss: 46361.3843.\n",
      "iteration i = \n",
      "20\n",
      "is ended and we are going to the next one\n",
      "-------------------------------------------\n",
      "epoch:  21 / 050, training loss: 46335.8011 , validation loss: 46323.1658.\n"
     ]
    }
   ],
   "source": [
    "epochs = 50\n",
    "for i in range(epochs):\n",
    "    loss_history['training'].append(0)\n",
    "    training_iterator.reset()\n",
    "    validation_iterator.reset()\n",
    "    for j, batch in enumerate(training_iterator):\n",
    "        with chainer.using_config('train', True):\n",
    "            x = chainer.Variable(dataset.concat_examples(batch, device))\n",
    "            \n",
    "            # These Should be deleted in the final version\n",
    "            mean_z, ln_var_z, mean_x, ln_var_x = model(x)\n",
    "            \n",
    "            # (1) start\n",
    "            loss = loss_fn(mean_z, ln_var_z, x, mean_x, ln_var_x)\n",
    "            model.cleargrads()\n",
    "            loss.backward()\n",
    "\n",
    "            optimizer.update()\n",
    "        \n",
    "        loss_history['training'][-1] += float(loss.data)\n",
    "    loss_history['training'][-1] /= j + 1\n",
    "    \n",
    "    loss_history['validation'].append(0)\n",
    "\n",
    "\n",
    "    for j1, batch1 in enumerate(validation_iterator):\n",
    "        with chainer.using_config('train', False):\n",
    "            x1 = chainer.Variable(dataset.concat_examples(batch1, device))\n",
    "            mean_z1, ln_var_z1, mean_x1, ln_var_x1 = model(x1)\n",
    "\n",
    "            loss = loss_fn(mean_z1, ln_var_z1,x1, mean_x1, ln_var_x1)\n",
    "            model.cleargrads()\n",
    "            loss.backward()\n",
    "\n",
    "            optimizer.update()\n",
    "        loss_history['validation'][-1] += float(loss.data)\n",
    "    loss_history['validation'][-1] /= j1 + 1\n",
    "    #print(\"loss_history['validation'] = \")\n",
    "    #print(loss_history['validation'])\n",
    "\n",
    "\n",
    "    print(\"iteration i = \")\n",
    "    print(i)\n",
    "    print(\"is ended and we are going to the next one\")\n",
    "    print(\"-------------------------------------------\")\n",
    "    #print('epoch: {:3d} / {:03d}, training loss: {:.4f}, validation loss: {:.4f}.'.format(i + 1,epochs,loss_history['training'],loss_history['validation']))\n",
    "    print('epoch: {:3d} / {:03d}, training loss: {:.4f} , validation loss: {:.4f}.'.format(i + 1, epochs, loss_history ['training'][i], loss_history['validation'][i]))\n",
    "    np.savez('{:s}/loss_history_{:03d}.npz'.format(model_directory, epochs), loss_history)\n",
    "    #np.savez('{:s}/loss_history_{:03d}.npz'.format(model_directory, epoch), loss_history)\n",
    "    serializers.save_npz('groupV6.model', model)\n",
    "\n",
    "    serializers.save_npz('{:s}/optimizer_{:03d}.npz'.format(model_directory, epochs), optimizer)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TESTING\n",
    "------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "loss_history['test']=[]\n",
    "loss_history['test'].append(0)\n",
    "#ourModel=Model(3, outsize)\n",
    "ourModel =Model(Decoder(), Encoder()).to_cpu()\n",
    "\n",
    "serializers.load_npz('groupV5.model'.format(model_directory),ourModel)\n",
    "\n",
    "\n",
    "for k, batch1 in enumerate(test_iterator):\n",
    "    with chainer.using_config('test', False):\n",
    "        print(\"-----------------------------------------\")\n",
    "        print(\" k = \")\n",
    "        print(k)\n",
    "        x1 = dataset.concat_examples(batch1)\n",
    "        mean_z1, ln_var_z1, mean_x1, ln_var_x1 = model(x1)\n",
    "        print(mean_x1)\n",
    "        x1_res=np.reshape(x1,[-1,28,28])\n",
    "        \n",
    "        #rmean_x1 = np.array(mean_x1)\n",
    "        rmean_x1=mean_x1.array\n",
    "        print(type(rmean_x1))\n",
    "        r_mean_x1 = np.reshape(rmean_x1,[-1,28,28])\n",
    "        print(r_mean_x1.shape)\n",
    "        \n",
    "        plt.figure(k)\n",
    "        plt.subplot(2,2,1)\n",
    "        plt.imshow(x1_res[1][:,:])\n",
    "        plt.subplot(2,2,2)\n",
    "        plt.imshow(r_mean_x1[1][:,:])\n",
    "        \n",
    "        \n",
    "        \n",
    "        loss = loss_fn(mean_z1, ln_var_z1,x1, mean_x1, ln_var_x1)\n",
    "        model.cleargrads()\n",
    "        loss.backward()        \n",
    "    loss_history['test'][-1]+=float(loss.data)\n",
    "   \n",
    "loss_history['test'][-1]/=k+1\n",
    "print(loss_history['test'][-1])\n",
    "np.savez('{:s}/test_loss_history.npz'.format(model_directory), loss_history['test'])\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "EzCbq-7SKmBW"
   },
   "source": [
    "Bonus task: Try the same experiments on a different dataset. (20 bonus points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "BONUS TASK - Fashion MNIST Dataset\n",
    "--------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#chainer.datasets.get_fashion_mnist(withlabel=True, ndim=1, scale=1.0, dtype=None, label_dtype=<class 'numpy.int32'>, rgb_format=False)\n",
    "class fashionMnist(DatasetMixin):\n",
    "    def __init__(self):\n",
    "        self.dataset = chainer.datasets.get_fashion_mnist(False)[0 if chainer.config.train else 1]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "\n",
    "    def get_example(self, i):\n",
    "        return self.dataset[i]\n",
    "    \n",
    "    # The MNIST database is a dataset of handwritten digits. \n",
    "    # It has 60,000 training samples, and 10,000 test samples. \n",
    "    # Each image is represented by 28x28 pixels, \n",
    "    # each containing a value 0 - 255 with its grayscale value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Dataset_f = fashionMnist()\n",
    "device = 0\n",
    "loss_fn_f = Loss()\n",
    "batch_size = 64\n",
    "#model_directory = 'Model'\n",
    "model_directory_f = 'C:\\\\Users\\\\Αngeliki-Ilektra\\\\NeuralInformationProcessingSystems\\\\3rdAs\\\\modelDirectory_f'\n",
    "model_f = Model(Decoder(), Encoder()).to_cpu()\n",
    "#vae = VAE()\n",
    "#root=\n",
    "training_set_f = Dataset_f[:int(0.8*55000)]\n",
    "validation_set_f = Dataset_f[int(0.8*55000):55000]\n",
    "#validation_set = Dataset[int(0.01*55000):int(0.011*55000)]\n",
    "test_set_f = Dataset_f[55000:len(Dataset)]\n",
    "\n",
    "loss_history_f = {'training_f': [], 'validation_f': []}\n",
    "\n",
    "\n",
    "training_iterator_f = iterators.SerialIterator(training_set_f, batch_size, False, True)\n",
    "print(enumerate(training_iterator))\n",
    "validation_iterator_f = iterators.SerialIterator(validation_set_f, batch_size, False, False)\n",
    "test_iterator_f = iterators.SerialIterator(test_set_f, batch_size, False, False)\n",
    "\n",
    "#vae.model.to_cpu()\n",
    "epochs=2\n",
    "optimizer = chainer.optimizers.Adam()\n",
    "optimizer.setup(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 1\n",
    "for i in range(epochs):\n",
    "    loss_history_f['training'].append(0)\n",
    "    training_iterator_f.reset()\n",
    "    validation_iterator_f.reset()\n",
    "    for j, batch in enumerate(training_iterator_f):\n",
    "        with chainer.using_config('train', True):\n",
    "            x = chainer.Variable(dataset_f.concat_examples(batch))\n",
    "            \n",
    "            # These Should be deleted in the final version\n",
    "            mean_z, ln_var_z, mean_x, ln_var_x = model_f(x)\n",
    "            \n",
    "            # (1) start\n",
    "            loss_f = loss_fn(mean_z, ln_var_z, x, mean_x, ln_var_x)\n",
    "            model_f.cleargrads()\n",
    "            loss_f.backward()\n",
    "\n",
    "            optimizer.update()\n",
    "        \n",
    "        loss_history_f['training'][-1] += float(loss_f.data)\n",
    "    loss_history_f['training'][-1] /= j + 1\n",
    "    \n",
    "    loss_history['validation'].append(0)\n",
    "\n",
    "\n",
    "    for j1, batch1 in enumerate(validation_iterator_f):\n",
    "        with chainer.using_config('train', False):\n",
    "            x1 = dataset_f.concat_examples(batch1)\n",
    "            mean_z1, ln_var_z1, mean_x1, ln_var_x1 = model_f(x1)\n",
    "\n",
    "            loss_f = loss_fn(mean_z1, ln_var_z1,x1, mean_x1, ln_var_x1)\n",
    "            model_f.cleargrads()\n",
    "            loss_f.backward()\n",
    "\n",
    "            optimizer.update()\n",
    "        loss_history_f['validation'][-1] += float(loss_f.data)\n",
    "    loss_history_f['validation'][-1] /= j1 + 1\n",
    "    #print(\"loss_history['validation'] = \")\n",
    "    #print(loss_history['validation'])\n",
    "\n",
    "\n",
    "    print(\"iteration i = \")\n",
    "    print(i)\n",
    "    print(\"is ended and we are going to the next one\")\n",
    "    print(\"-------------------------------------------\")\n",
    "    #print('epoch: {:3d} / {:03d}, training loss: {:.4f}, validation loss: {:.4f}.'.format(i + 1,epochs,loss_history['training'],loss_history['validation']))\n",
    "    print('epoch: {:3d} / {:03d}, training loss: {} , validation loss: {}.'.format(i + 1, epochs, loss_history ['training'], loss_history['validation']))\n",
    "    np.savez('{:s}/loss_history_{:03d}.npz'.format(model_directory, epochs), loss_history)\n",
    "    #np.savez('{:s}/loss_history_{:03d}.npz'.format(model_directory, epoch), loss_history)\n",
    "    serializers.save_npz('groupV1.model', model)\n",
    "\n",
    "    serializers.save_npz('{:s}/optimizer_{:03d}.npz'.format(model_directory, epochs), optimizer)\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "weeks_6_and_7",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
